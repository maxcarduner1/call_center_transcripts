{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2026dfc8-763d-486a-a098-54cffed60919",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Databricks: Author and deploy an MCP tool-calling LangGraph agent\n",
    "\n",
    "This notebook shows how to author a LangGraph agent that connects to MCP servers hosted on Databricks. LangGraph's graph-based architecture gives you complete control over agent behavior, making it the right choice when you need custom workflows or multi-step reasoning patterns.\n",
    "\n",
    "Connect your agent to data and tools through MCP servers. Databricks provides managed MCP servers for Unity Catalog functions, vector search, and Genie spaces. You can also connect to custom MCP servers that you host as Databricks Apps. See [MCP on Databricks](https://docs.databricks.com/aws/en/generative-ai/mcp/).\n",
    "\n",
    "In this notebook, you:\n",
    "\n",
    "- Author a LangGraph agent\n",
    "- Connect the agent to MCP servers to access Databricks-hosted tools\n",
    "- Test the agent and evaluate its responses using MLflow Evaluation\n",
    "- Log the agent with MLflow and deploy it to a model serving endpoint\n",
    "\n",
    "This notebook uses the  [`ResponsesAgent`](https://mlflow.org/docs/latest/api_reference/python_api/mlflow.pyfunc.html#mlflow.pyfunc.ResponsesAgent) for Databrick compatibility.\n",
    "\n",
    "To learn more about authoring an agent using Mosaic AI Agent Framework, see Databricks documentation ([AWS](https://docs.databricks.com/aws/generative-ai/agent-framework/author-agent) | [Azure](https://learn.microsoft.com/azure/databricks/generative-ai/agent-framework/create-chat-model)).\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Address all `TODO`s in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c107c2c4-df71-4402-b82f-82e2f2d64399",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U -qqqq --force-reinstall databricks-langchain databricks-agents uv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c715704-ad67-45c9-a870-65728a01f38e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18db682c-b2a3-48b0-9e91-b2b811419531",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Define the agent code\n",
    "\n",
    "Define the agent code in a single cell below. This lets you easily write the agent\n",
    "code to a local Python file, using the `%%writefile` magic command, for subsequent\n",
    "logging and deployment.\n",
    "\n",
    "**What this code does at a high level:**\n",
    "\n",
    "1. **Connect to MCP servers using adapters**\n",
    "    The `DatabricksMCPServer` and `DatabricksMultiServerMCPClient` from `databricks_langchain` handle:\n",
    "    - Connections to Databricks MCP servers\n",
    "    - Authentication\n",
    "    - Automatic tool discovery and conversion to LangChain-compatible format\n",
    "\n",
    "2. **Build a LangGraph agent workflow using LangGraph `StateGraph`**\n",
    "\n",
    "3. **Handle streaming responses**\n",
    "    The `MCPToolCallingAgent` class wraps the LangGraph workflow to:\n",
    "    - Process streaming events from the agent graph in real-time\n",
    "    - Convert LangChain message formats to Mosaic AI-compatible format\n",
    "    - Enable MLflow tracing for each step of the agent workflow\n",
    "\n",
    "4. **Wrap with ResponsesAgent**\n",
    "    The agent is wrapped using `ResponsesAgent` for compatibility with Databricks\n",
    "    features like evaluation, deployment, and feedback collection.\n",
    "\n",
    "5. **MLflow autotracing**\n",
    "    Enable MLflow autologging to automatically trace LLM calls, tool invocations,\n",
    "    and agent state transitions.\n",
    "\n",
    "#### Agent tools\n",
    "\n",
    "This example connects to the Unity Catalog functions MCP server to access\n",
    "`system.ai.python_exec` (a built-in Python code interpreter). The code also\n",
    "includes commented-out examples for connecting to:\n",
    "- Custom MCP servers (hosted as Databricks Apps)\n",
    "- Vector search MCP servers (for semantic search over your data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a440f1ad-f51c-4f69-a081-94d8b2fa722d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%writefile agent.py\n",
    "\n",
    "import asyncio\n",
    "from typing import Annotated, Any, AsyncGenerator, Generator, Optional, Sequence, TypedDict, Union\n",
    "\n",
    "import mlflow\n",
    "import nest_asyncio\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks_langchain import (\n",
    "    ChatDatabricks,\n",
    "    DatabricksMCPServer,\n",
    "    DatabricksMultiServerMCPClient,\n",
    ")\n",
    "from langchain.messages import AIMessage, AIMessageChunk, AnyMessage\n",
    "from langchain_core.language_models import LanguageModelLike\n",
    "from langchain_core.runnables import RunnableConfig, RunnableLambda\n",
    "from langchain_core.tools import BaseTool\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt.tool_node import ToolNode\n",
    "from mlflow.pyfunc import ResponsesAgent\n",
    "from mlflow.types.responses import (\n",
    "    ResponsesAgentRequest,\n",
    "    ResponsesAgentResponse,\n",
    "    ResponsesAgentStreamEvent,\n",
    "    output_to_responses_items_stream,\n",
    "    to_chat_completions_input,\n",
    ")\n",
    "from langchain_core.messages.tool import ToolMessage\n",
    "import json\n",
    "\n",
    "nest_asyncio.apply()\n",
    "############################################\n",
    "## Define your LLM endpoint and system prompt\n",
    "############################################\n",
    "LLM_ENDPOINT_NAME = \"databricks-claude-sonnet-4-5\"\n",
    "llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME)\n",
    "\n",
    "# TODO: Update with your system prompt\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant that can help answer questions about a call center rep's performance\n",
    "\"\"\"\n",
    "\n",
    "# TODO: Choose your MCP server connection type and setup the Workspace Clients for Authentication\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Managed MCP Server — simplest setup\n",
    "# ---------------------------------------------------------------------------\n",
    "# Databricks manages this connection automatically using your workspace settings\n",
    "# and Personal Access Token (PAT) authentication.\n",
    "\n",
    "workspace_client = WorkspaceClient()\n",
    "\n",
    "host = workspace_client.config.host\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Custom MCP Server — hosted as a Databricks App\n",
    "# ---------------------------------------------------------------------------\n",
    "# Use this if you’re running your own MCP server in Databricks.\n",
    "# These require OAuth with a service principal for machine-to-machine (M2M) auth.\n",
    "#\n",
    "# Follow the insturctions here in order to create a SP, grant the SP query permissions on your app and then mint a client id and # secret. https://docs.databricks.com/aws/en/dev-tools/auth/oauth-m2m\n",
    "#\n",
    "# Uncomment and fill in the settings below to use a custom MCP server.\n",
    "#\n",
    "# import os\n",
    "# custom_mcp_server_workspace_client = WorkspaceClient(\n",
    "#     host=\"<DATABRICKS_WORKSPACE_URL>\",\n",
    "#     client_id=os.getenv(\"DATABRICKS_CLIENT_ID\"),\n",
    "#     client_secret=os.getenv(\"DATABRICKS_CLIENT_SECRET\"),\n",
    "#     auth_type=\"oauth-m2m\",  # Enables service principal authentication\n",
    "# )\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# OBO Setup\n",
    "# ---------------------------------------------------------------------------\n",
    "# In order to use OBO, uncomment the code below and pass this workspace client to the appropriate McpServer below\n",
    "#\n",
    "# from databricks_ai_bridge import ModelServingUserCredentials\n",
    "# obo_workspace_client = WorkspaceClient(credentials_strategy=ModelServingUserCredentials())\n",
    "\n",
    "###############################################################################\n",
    "## Configure MCP Servers for your agent\n",
    "##\n",
    "## This section sets up server connections so your agent can retrieve data or take actions.\n",
    "\n",
    "## There are three connection types:\n",
    "## 1. Managed MCP servers — fully managed by Databricks\n",
    "## 2. External MCP servers — hosted outside Databricks but proxied through a\n",
    "##    Managed MCP server proxy\n",
    "## 3. Custom MCP servers — MCP servers hosted as Databricks Apps\n",
    "##\n",
    "###############################################################################\n",
    "databricks_mcp_client = DatabricksMultiServerMCPClient(\n",
    "    [\n",
    "        DatabricksMCPServer(\n",
    "            name=\"system-ai\",\n",
    "            url=f\"{host}/api/2.0/mcp/functions/telco_call_center/analytics\",\n",
    "        ),\n",
    "        # DatabricksMCPServer(\n",
    "        #     name=\"custom_mcp\",\n",
    "        #     url=\"custom_app_url\",\n",
    "        #     workspace_client=custom_mcp_server_workspace_client\n",
    "        # ),\n",
    "        # DatabricksMCPServer(\n",
    "        #     name=\"obo_vs_client\",\n",
    "        #     url=f\"{host}/api/2.0/mcp/vector-search/system/ai\",\n",
    "        #     workspace_client=obo_workspace_client\n",
    "        # )\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# The state for the agent workflow, including the conversation and any custom data\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[AnyMessage], add_messages]\n",
    "    custom_inputs: Optional[dict[str, Any]]\n",
    "    custom_outputs: Optional[dict[str, Any]]\n",
    "\n",
    "\n",
    "def create_tool_calling_agent(\n",
    "    model: LanguageModelLike,\n",
    "    tools: Union[ToolNode, Sequence[BaseTool]],\n",
    "    system_prompt: Optional[str] = None,\n",
    "):\n",
    "    model = model.bind_tools(tools)  # Bind tools to the model\n",
    "\n",
    "    # Function to check if agent should continue or finish based on last message\n",
    "    def should_continue(state: AgentState):\n",
    "        messages = state[\"messages\"]\n",
    "        last_message = messages[-1]\n",
    "        # If function (tool) calls are present, continue; otherwise, end\n",
    "        if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "            return \"continue\"\n",
    "        else:\n",
    "            return \"end\"\n",
    "\n",
    "    # Preprocess: optionally prepend a system prompt to the conversation history\n",
    "    if system_prompt:\n",
    "        preprocessor = RunnableLambda(\n",
    "            lambda state: [{\"role\": \"system\", \"content\": system_prompt}] + state[\"messages\"]\n",
    "        )\n",
    "    else:\n",
    "        preprocessor = RunnableLambda(lambda state: state[\"messages\"])\n",
    "\n",
    "    model_runnable = preprocessor | model  # Chain the preprocessor and the model\n",
    "\n",
    "    # The function to invoke the model within the workflow\n",
    "    def call_model(\n",
    "        state: AgentState,\n",
    "        config: RunnableConfig,\n",
    "    ):\n",
    "        response = model_runnable.invoke(state, config)\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "    workflow = StateGraph(AgentState)  # Create the agent's state machine\n",
    "\n",
    "    workflow.add_node(\"agent\", RunnableLambda(call_model))  # Agent node (LLM)\n",
    "    workflow.add_node(\"tools\", ToolNode(tools))  # Tools node\n",
    "\n",
    "    workflow.set_entry_point(\"agent\")  # Start at agent node\n",
    "    workflow.add_conditional_edges(\n",
    "        \"agent\",\n",
    "        should_continue,\n",
    "        {\n",
    "            \"continue\": \"tools\",  # If the model requests a tool call, move to tools node\n",
    "            \"end\": END,  # Otherwise, end the workflow\n",
    "        },\n",
    "    )\n",
    "    workflow.add_edge(\"tools\", \"agent\")  # After tools are called, return to agent node\n",
    "\n",
    "    # Compile and return the tool-calling agent workflow\n",
    "    return workflow.compile()\n",
    "\n",
    "\n",
    "# ResponsesAgent class to wrap the compiled agent and make it compatible with Mosaic AI Responses API\n",
    "class LangGraphResponsesAgent(ResponsesAgent):\n",
    "    def __init__(self, agent):\n",
    "        self.agent = agent\n",
    "\n",
    "    # Make a prediction (single-step) for the agent\n",
    "    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n",
    "        outputs = [\n",
    "            event.item\n",
    "            for event in self.predict_stream(request)\n",
    "            if event.type == \"response.output_item.done\" or event.type == \"error\"\n",
    "        ]\n",
    "        return ResponsesAgentResponse(output=outputs, custom_outputs=request.custom_inputs)\n",
    "\n",
    "    async def _predict_stream_async(\n",
    "        self,\n",
    "        request: ResponsesAgentRequest,\n",
    "    ) -> AsyncGenerator[ResponsesAgentStreamEvent, None]:\n",
    "        cc_msgs = to_chat_completions_input([i.model_dump() for i in request.input])\n",
    "        # Stream events from the agent graph\n",
    "        async for event in self.agent.astream(\n",
    "            {\"messages\": cc_msgs}, stream_mode=[\"updates\", \"messages\"]\n",
    "        ):\n",
    "            if event[0] == \"updates\":\n",
    "                # Stream updated messages from the workflow nodes\n",
    "                for node_data in event[1].values():\n",
    "                    if len(node_data.get(\"messages\", [])) > 0:\n",
    "                        all_messages = []\n",
    "                        for msg in node_data[\"messages\"]:\n",
    "                            if isinstance(msg, ToolMessage) and not isinstance(msg.content, str):\n",
    "                                msg.content = json.dumps(msg.content)\n",
    "                            all_messages.append(msg)\n",
    "                        for item in output_to_responses_items_stream(all_messages):\n",
    "                            yield item\n",
    "            elif event[0] == \"messages\":\n",
    "                # Stream generated text message chunks\n",
    "                try:\n",
    "                    chunk = event[1][0]\n",
    "                    if isinstance(chunk, AIMessageChunk) and (content := chunk.content):\n",
    "                        yield ResponsesAgentStreamEvent(\n",
    "                            **self.create_text_delta(delta=content, item_id=chunk.id),\n",
    "                        )\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "    # Stream predictions for the agent, yielding output as it's generated\n",
    "    def predict_stream(\n",
    "        self, request: ResponsesAgentRequest\n",
    "    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
    "        agen = self._predict_stream_async(request)\n",
    "\n",
    "        try:\n",
    "            loop = asyncio.get_event_loop()\n",
    "        except RuntimeError:\n",
    "            loop = asyncio.new_event_loop()\n",
    "            asyncio.set_event_loop(loop)\n",
    "\n",
    "        ait = agen.__aiter__()\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                item = loop.run_until_complete(ait.__anext__())\n",
    "            except StopAsyncIteration:\n",
    "                break\n",
    "            else:\n",
    "                yield item\n",
    "\n",
    "\n",
    "# Initialize the entire agent, including MCP tools and workflow\n",
    "def initialize_agent():\n",
    "    \"\"\"Initialize the agent with MCP tools\"\"\"\n",
    "    # Create MCP tools from the configured servers\n",
    "    mcp_tools = asyncio.run(databricks_mcp_client.get_tools())\n",
    "\n",
    "    # Create the agent graph with an LLM, tool set, and system prompt (if given)\n",
    "    agent = create_tool_calling_agent(llm, mcp_tools, system_prompt)\n",
    "    return LangGraphResponsesAgent(agent)\n",
    "\n",
    "\n",
    "mlflow.langchain.autolog()\n",
    "AGENT = initialize_agent()\n",
    "mlflow.models.set_model(AGENT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e95c42b9-7cd5-4a76-a5c2-cf6ed7af87e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Test the agent\n",
    "\n",
    "Interact with the agent to test its output and tool-calling abilities. Since this notebook called `mlflow.langchain.autolog()`, you can view the trace for each step the agent takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "076c0e67-b4ab-45b0-bb13-5b7952402270",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b9394fd-d664-4f5e-9efd-8ff6b4ef3814",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TODO: ONLY UNCOMMENT AND EDIT THIS SECTION IF YOU ARE USING OAUTH/SERVICE PRINCIPAL FOR CUSTOM MCP SERVERS.\n",
    "#       For managed MCP (the default), LEAVE THIS SECTION COMMENTED OUT.\n",
    "# ==============================================================================\n",
    "\n",
    "# import os\n",
    "\n",
    "# # Set your Databricks client ID and client secret for service principal authentication.\n",
    "# DATABRICKS_CLIENT_ID = \"<YOUR_CLIENT_ID>\"\n",
    "# client_secret_scope_name = \"<YOUR_SECRET_SCOPE>\"\n",
    "# client_secret_key_name = \"<YOUR_SECRET_KEY_NAME>\"\n",
    "\n",
    "# # Load your service principal credentials into environment variables\n",
    "# os.environ[\"DATABRICKS_CLIENT_ID\"] = DATABRICKS_CLIENT_ID\n",
    "# os.environ[\"DATABRICKS_CLIENT_SECRET\"] = dbutils.secrets.get(scope=client_secret_scope_name, key=client_secret_key_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6da1040-45b9-4e8e-9640-e2b64931c1a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from agent import AGENT\n",
    "\n",
    "AGENT.predict({\"input\": [{\"role\": \"user\", \"content\": \"How is the customer service rep Maya doing?\"}]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10003231-75e0-4f15-b453-34d16e46077e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for chunk in AGENT.predict_stream(\n",
    "    {\"input\": [{\"role\": \"user\", \"content\": \"What is 7*6 in Python?\"}]}\n",
    "):\n",
    "    print(chunk, \"-----------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ce59709-8c40-49f0-8e04-f8307b5ac1cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Log the agent as an MLflow model\n",
    "\n",
    "Log the agent as code from the `agent.py` file. See [Deploy an agent that connects to Databricks MCP servers](https://docs.databricks.com/aws/en/generative-ai/mcp/managed-mcp#deploy-your-agent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34ac82c9-1bda-4e14-a579-c3fba00f0123",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from agent import LLM_ENDPOINT_NAME\n",
    "from mlflow.models.resources import DatabricksServingEndpoint, DatabricksFunction\n",
    "from pkg_resources import get_distribution\n",
    "\n",
    "resources = [\n",
    "    DatabricksServingEndpoint(endpoint_name=LLM_ENDPOINT_NAME), \n",
    "    DatabricksFunction(function_name=\"system.ai.python_exec\")\n",
    "]\n",
    "\n",
    "with mlflow.start_run():\n",
    "    logged_agent_info = mlflow.pyfunc.log_model(\n",
    "        name=\"agent\",\n",
    "        python_model=\"agent.py\",\n",
    "        resources=resources,\n",
    "        pip_requirements=[\n",
    "            f\"langgraph=={get_distribution('langgraph').version}\",\n",
    "            f\"mcp=={get_distribution('mcp').version}\",\n",
    "            f\"databricks-mcp=={get_distribution('databricks-mcp').version}\",\n",
    "            f\"databricks-langchain=={get_distribution('databricks-langchain').version}\",\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "acdd669d-7ade-4ff1-a5d8-1f540948d57a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Evaluate the agent with [Agent Evaluation](https://docs.databricks.com/mlflow3/genai/eval-monitor)\n",
    "\n",
    "You can edit the requests or expected responses in your evaluation dataset and run evaluation as you iterate your agent, leveraging mlflow to track the computed quality metrics.\n",
    "\n",
    "Evaluate your agent with one of our [predefined LLM scorers](https://docs.databricks.com/mlflow3/genai/eval-monitor/predefined-judge-scorers), or try adding [custom metrics](https://docs.databricks.com/mlflow3/genai/eval-monitor/custom-scorers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0266ee29-1fc8-41cc-84c0-63ffcef9f392",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.genai.scorers import RelevanceToQuery, Safety, RetrievalRelevance, RetrievalGroundedness\n",
    "\n",
    "eval_dataset = [\n",
    "    {\n",
    "        \"inputs\": {\n",
    "            \"input\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"Calculate the 15th Fibonacci number\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        \"expected_response\": \"The 15th Fibonacci number is 610.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "eval_results = mlflow.genai.evaluate(\n",
    "    data=eval_dataset,\n",
    "    predict_fn=lambda input: AGENT.predict({\"input\": input}),\n",
    "    scorers=[RelevanceToQuery(), Safety()], # add more scorers here if they're applicable\n",
    ")\n",
    "\n",
    "# Review the evaluation results in the MLfLow UI (see console output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95ffc536-f9c1-45b0-890c-9b382c9552ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.models.predict(\n",
    "    model_uri=f\"runs:/{logged_agent_info.run_id}/agent\",\n",
    "    input_data={\"input\": [{\"role\": \"user\", \"content\": \"What is 7*6 in Python?\"}]},\n",
    "    env_manager=\"uv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3989e7dd-c714-4ea5-ac33-e50b6b8e22ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Register the model to Unity Catalog\n",
    "\n",
    "Before you deploy the agent, you must register the agent to Unity Catalog.\n",
    "\n",
    "- **TODO** Update the `catalog`, `schema`, and `model_name` below to register the MLflow model to Unity Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c22837d-2b8c-4f92-8653-135268632270",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "# TODO: define the catalog, schema, and model name for your UC model\n",
    "catalog = \"\"\n",
    "schema = \"\"\n",
    "model_name = \"\"\n",
    "UC_MODEL_NAME = f\"{catalog}.{schema}.{model_name}\"\n",
    "\n",
    "# register the model to UC\n",
    "uc_registered_model_info = mlflow.register_model(\n",
    "    model_uri=logged_agent_info.model_uri, name=UC_MODEL_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d23daef3-24d6-4708-a01d-5d42ca861384",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Deploy the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f95f3fb0-c5d3-473e-87c0-c34352fb81e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks import agents\n",
    "\n",
    "agents.deploy(\n",
    "    UC_MODEL_NAME, \n",
    "    uc_registered_model_info.version,\n",
    "    # ==============================================================================\n",
    "    # TODO: ONLY UNCOMMENT AND CONFIGURE THE ENVIRONMENT_VARS SECTION BELOW\n",
    "    #       IF YOU ARE USING OAUTH/SERVICE PRINCIPAL FOR CUSTOM MCP SERVERS.\n",
    "    #       For managed MCP (the default), LEAVE THIS SECTION COMMENTED OUT.\n",
    "    # ==============================================================================\n",
    "    # environment_vars={\n",
    "    #     \"DATABRICKS_CLIENT_ID\": DATABRICKS_CLIENT_ID,\n",
    "    #     \"DATABRICKS_CLIENT_SECRET\": f\"{{{{secrets/{client_secret_scope_name}/{client_secret_key_name}}}}}\"\n",
    "    # },\n",
    "    tags = {\"endpointSource\": \"docs\"},\n",
    "    deploy_feedback_model=False\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "langgraph-mcp-tool-calling-agent",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
