{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b0dcfae-d8ba-4cf4-9b00-d250c8df271e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0604d935-ffae-4d4d-bdcb-9e87171adead",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 1"
    }
   },
   "outputs": [],
   "source": [
    "# Display sample records from historical_telephonic_outreach table\n",
    "display(spark.table(\"historical_telephonic_outreach\").limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "295deff8-4a28-4c86-946a-0e38a7e41426",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Show distribution of historical telephonic outreach by call_outcome\n",
    "df = spark.table(\"historical_telephonic_outreach\")\n",
    "distribution_df = df.groupBy(\"call_outcome\").count()\n",
    "display(distribution_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "490cf92b-4e60-4c14-98d1-f17c05be5d1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display sample records where call_outcome is 'connected-completed'\n",
    "df = spark.table(\"historical_telephonic_outreach\")\n",
    "sample_df = df.filter(df.call_outcome == \"Connected - Completed\").limit(10)\n",
    "display(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3603f924-bd58-4d5d-af04-edac38ee4cd8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 5"
    }
   },
   "outputs": [],
   "source": [
    "# Sample 10 member_ids from historical_telephonic_outreach\n",
    "df = spark.table(\"historical_telephonic_outreach\")\n",
    "sample_member_ids = df.select(\"member_id\").distinct().limit(30)\n",
    "display(sample_member_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4972503-43d5-43ae-89ae-2738c9db11e6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Filter records for sampled member_ids"
    }
   },
   "outputs": [],
   "source": [
    "# Filter the entire table to show only records for the 10 sampled member_ids\n",
    "member_ids_list = [row.member_id for row in sample_member_ids.collect()]\n",
    "filtered_df = df.filter(df.member_id.isin(member_ids_list))\n",
    "\n",
    "print(f\"Total records for {len(member_ids_list)} member_ids: {filtered_df.count()}\")\n",
    "display(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f80409d5-efda-498b-a87b-515ce4dfeed7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Show distribution of call_outcomes for the filtered member_ids\n",
    "outcome_distribution = filtered_df.groupBy(\"call_purpose\").count().orderBy(\"count\", ascending=False)\n",
    "display(outcome_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6181b415-a950-49d9-9fab-40d20d430745",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create telco call purpose lookup table"
    }
   },
   "outputs": [],
   "source": [
    "# Create lookup table mapping healthcare to telco call purposes\n",
    "telco_lookup = [\n",
    "    (\"Chronic Care Management\", \"5G Upgrade Opportunity\"),\n",
    "    (\"Transportation Assistance\", \"Device Trade-In Program\"),\n",
    "    (\"Health Risk Assessment\", \"Plan Optimization Review\"),\n",
    "    (\"Benefits Education\", \"Family Plan Expansion\"),\n",
    "    (\"Medication Adherence\", \"International Roaming Package\"),\n",
    "    (\"Care Gap Closure\", \"Device Protection Plan\"),\n",
    "    (\"Social Services Referral\", \"Data Overage Prevention\"),\n",
    "    (\"Post-Discharge Follow-up\", \"Loyalty Rewards Redemption\"),\n",
    "    (\"Annual Wellness Visit Scheduling\", \"New Service Bundle Offer\")\n",
    "]\n",
    "\n",
    "# Create DataFrame from lookup table\n",
    "telco_lookup_df = spark.createDataFrame(telco_lookup, [\"healthcare_purpose\", \"telco_purpose\"])\n",
    "display(telco_lookup_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4b25d7b-d65d-423a-bd49-27f5fc29a67d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Swap call purposes in filtered_df"
    }
   },
   "outputs": [],
   "source": [
    "# Join filtered_df with lookup table to swap call purposes\n",
    "from pyspark.sql.functions import coalesce\n",
    "\n",
    "filtered_df_telco = filtered_df.join(\n",
    "    telco_lookup_df, \n",
    "    filtered_df.call_purpose == telco_lookup_df.healthcare_purpose, \n",
    "    \"left\"\n",
    ").withColumn(\n",
    "    \"call_purpose\", \n",
    "    coalesce(telco_lookup_df.telco_purpose, filtered_df.call_purpose)\n",
    ").drop(\"healthcare_purpose\", \"telco_purpose\")\n",
    "\n",
    "print(f\"Total records with telco purposes: {filtered_df_telco.count()}\")\n",
    "display(filtered_df_telco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3771589c-c14e-4a1c-a9cf-5129340e869e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create rep name mapping"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import row_number, concat, lpad, lit, col\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Get distinct agent_ids from filtered_df_telco\n",
    "distinct_agents = filtered_df_telco.select(\"agent_id\").distinct().orderBy(\"agent_id\")\n",
    "\n",
    "# Create fake names for 5 reps\n",
    "fake_names = [\n",
    "    \"Sarah Mitchell\",\n",
    "    \"James Rodriguez\",\n",
    "    \"Emily Chen\",\n",
    "    \"Michael Thompson\",\n",
    "    \"Jessica Martinez\"\n",
    "]\n",
    "\n",
    "# Collect agent_ids and create mapping with both rep_id and rep_name\n",
    "agent_list = [row.agent_id for row in distinct_agents.collect()]\n",
    "rep_mapping = [(agent_id, f\"REP{str(i % 5 + 1).zfill(8)}\", fake_names[i % len(fake_names)]) for i, agent_id in enumerate(agent_list)]\n",
    "\n",
    "# Create DataFrame from mapping\n",
    "rep_mapping_df = spark.createDataFrame(rep_mapping, [\"agent_id\", \"rep_id\", \"rep_name\"])\n",
    "display(rep_mapping_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fcab33b-0fe5-4367-9d7d-d9ef62094e89",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Join rep mapping to filtered_df_telco"
    }
   },
   "outputs": [],
   "source": [
    "# Join rep mapping to filtered_df_telco\n",
    "filtered_df_telco_with_reps = filtered_df_telco.join(\n",
    "    rep_mapping_df,\n",
    "    on=\"agent_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "print(f\"Total records with rep info: {filtered_df_telco_with_reps.count()}\")\n",
    "display(filtered_df_telco_with_reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57a10be3-8503-4554-a8a0-b4e220a7de15",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Call outcome distribution for sampled members"
    }
   },
   "outputs": [],
   "source": [
    "# Show distribution of call_outcomes for the filtered member_ids\n",
    "outcome_distribution = filtered_df.groupBy(\"call_outcome\").count().orderBy(\"count\", ascending=False)\n",
    "display(outcome_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ba95736-fe8b-4ca0-add4-7758c10cadec",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Generate transcripts using AI_Query (Python)"
    }
   },
   "outputs": [],
   "source": [
    "# Generate fake transcripts for connected calls and voicemails using AI_Query\n",
    "from pyspark.sql.functions import col, concat, lit, when\n",
    "\n",
    "# Filter for Connected outcomes and Voicemail Left\n",
    "transcript_df = filtered_df_telco_with_reps.filter(\n",
    "    (col(\"call_outcome\").like(\"%Connected%\")) | (col(\"call_outcome\") == \"Voicemail Left\")\n",
    ")\n",
    "\n",
    "# Create the SQL query with ai_query\n",
    "transcript_query = \"\"\"\n",
    "SELECT \n",
    "  call_id,\n",
    "  member_id,\n",
    "  rep_id,\n",
    "  rep_name,\n",
    "  call_date,\n",
    "  call_time,\n",
    "  call_outcome,\n",
    "  call_purpose,\n",
    "  call_duration_seconds,\n",
    "  ai_query(\n",
    "    'databricks-gpt-5-2',\n",
    "    CASE \n",
    "      WHEN call_outcome = 'Voicemail Left' THEN\n",
    "        CONCAT(\n",
    "          'Generate a realistic voicemail message from a Telco call center representative named ', rep_name, ' calling on behalf of organization \"Brickster Wireless\". ',\n",
    "          'The call purpose is: ', call_purpose, '. ',\n",
    "          'The voicemail should be approximately ', CAST(call_duration_seconds AS STRING), ' seconds long when spoken. ',\n",
    "          'Only include the representative speaking, no customer response. ',\n",
    "          'Format as: [Rep: message]'\n",
    "        )\n",
    "      ELSE\n",
    "        CONCAT(\n",
    "          'Generate a realistic phone conversation transcript between a Telco call center representative named ', rep_name, ' from \"Brickster Wireless\" and a customer. ',\n",
    "          'The call purpose is: ', call_purpose, '. ',\n",
    "          'The call outcome was: ', call_outcome, '. ',\n",
    "          'The conversation should be approximately ', CAST(call_duration_seconds AS STRING), ' seconds long when spoken. ',\n",
    "          'Format as alternating lines with [Rep: ...] and [Customer: ...]'\n",
    "        )\n",
    "    END\n",
    "  ) AS transcript\n",
    "FROM ({table_name})\n",
    "\"\"\"\n",
    "\n",
    "# Create temp view and run query\n",
    "transcript_df.createOrReplaceTempView(\"filtered_calls_temp\")\n",
    "result_df = spark.sql(transcript_query.format(table_name=\"filtered_calls_temp\"))\n",
    "\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bc29430-6da7-4981-b1cb-8eaf44df6ad7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_df.write.mode(\"overwrite\").saveAsTable(\"historical_telephonic_outreach_transcripts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60cc0cec-0877-4b52-adef-4ac4f57acd10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "COC_Scorecard_Prompt = \"\"\"\n",
    "ONLY RETURN ANSWERS IN JSON STRUCT DEFINED BELOW\n",
    "\n",
    "You are an analyst scoring how well call center support specialists are performing.  You will be given a transcript of a call and the following criteria to judge it by. \n",
    "\n",
    "Each criteria has skills within them that you associate a score with.\n",
    "\n",
    "Criteria 1: Technical Aspects\n",
    "1. The agent satisfied proper recording disclosure requirements throughout the call.\n",
    "  - If yes, then they get six points\n",
    "  - If no, then they get zero points\n",
    "2. The agent satisfied proper member authentication requirements \n",
    "  - If yes, then they get eight points\n",
    "  - If no, then they get zero points\n",
    "3. The agent satisfied call closing requirements appropriately \n",
    "  - If yes, then they get four points\n",
    "  - If no, then they get zero points\n",
    "\n",
    "Criteria 2: Quality of Service\n",
    "1. The agent maintained professionalism and employed soft skills throughout the call\n",
    "  - Rate on a scale of 1 to 10 where 1 is unprofessional and 10 is most professional\n",
    "2. The agent offered correct and accurate program information when necessary\n",
    "  - Rate on a scale of 1 to 10 with 1 being incorrect and 10 being correct\n",
    "3. The agent demonstrated a positive and helpful demeanor throughout the call\n",
    "  - Rate on a scale of 1 to 10 with 1 being negative and 10 being positive\n",
    "\n",
    "For the response, structure it so that it is a JSON object with the following keys:\n",
    "\n",
    "- \"criteria_1\": a dictionary with the following keys:\n",
    "  - \"technical_aspects\": a dictionary with the following keys:\n",
    "    - \"recording_disclosure\": a dictionary with the following keys:\n",
    "      - \"score\": an integer between 0 and 6\n",
    "    - \"member_authentication\": a dictionary with the following keys:\n",
    "      - \"score\": an integer between 0 and 8\n",
    "    - \"call_closing\": a dictionary with the following keys:\n",
    "      - \"score\": an integer between 0 and 4\n",
    "- \"criteria_2\": a dictionary with the following keys:\n",
    "  - \"quality_of_service\": a dictionary with the following keys:\n",
    "    - \"professionalism\": a dictionary with the following keys:\n",
    "      - \"score\": an integer between 1 and 10\n",
    "    - \"program_information\": a dictionary with the following keys:\n",
    "      - \"score\": an integer between 1 and 10\n",
    "    - \"demeanor\": a dictionary with the following keys:\n",
    "      - \"score\": an integer between 1 and 10\n",
    "- \"total_score\": an integer between 0 and 48\n",
    "\n",
    "Your response should ONLY be a JSON object with the above structure.\n",
    "\"\"\"\n",
    "\n",
    "returnType = \"\"\"\n",
    "STRUCT<\n",
    "  criteria_1: STRUCT<\n",
    "    technical_aspects: STRUCT<\n",
    "      recording_disclosure: STRUCT<score: INT>,\n",
    "      member_authentication: STRUCT<score: INT>,\n",
    "      call_closing: STRUCT<score: INT>\n",
    "    >\n",
    "  >,\n",
    "  criteria_2: STRUCT<\n",
    "    quality_of_service: STRUCT<\n",
    "      professionalism: STRUCT<score: INT>,\n",
    "      program_information: STRUCT<score: INT>,\n",
    "      demeanor: STRUCT<score: INT>\n",
    "    >\n",
    "  >,\n",
    "  total_score: INT\n",
    ">\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fec1a8b2-89ea-4074-b0a4-528a4a380898",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## improvement: structured outputs\n",
    "transcription_scored_df = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "  *,\n",
    "  ai_query(\n",
    "    'databricks-gpt-oss-20b',\n",
    "    '{COC_Scorecard_Prompt}\\nTranscript: ' || transcript\n",
    "  ) AS scorecard\n",
    "FROM historical_telephonic_outreach_transcripts\n",
    "\"\"\")\n",
    "# transcription_scored_df.write.mode(\"overwrite\").saveAsTable(\"historical_telephonic_outreach_transcripts_scored\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "562ce398-b8e2-413a-8c5e-79c9c8837eb4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 12"
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import from_json\n",
    "\n",
    "# display(transcription_scored_df.withColumn(\"scorecard\", from_json(\"scorecard\", returnType)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "795c04fa-52e9-410f-8ca8-23139e3a7503",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 13"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, col\n",
    "\n",
    "scored_with_json_df = transcription_scored_df.withColumn(\"scorecard_json\", from_json(\"scorecard\", returnType)).drop(\"scorecard\")\n",
    "scored_with_json_df = scored_with_json_df.withColumn(\"total_score\", col(\"scorecard_json.total_score\"))\n",
    "scored_with_json_df.write.option(\"mergeSchema\", \"true\").mode(\"overwrite\").saveAsTable(\"call_center_scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2fd4e0b-675e-4513-ab87-f4c3667e3fe4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "summary_df = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    *, \n",
    "    ai_summarize(transcript) AS transcript_summary\n",
    "FROM call_center_scores\n",
    "\"\"\")\n",
    "\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "811441be-9269-4e60-ab06-53ae2bfb8106",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "summary_df.write.option(\"mergeSchema\", \"true\").mode(\"overwrite\").saveAsTable(\"call_center_scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b23ea6d-627e-4433-b2fe-a68d4ad4aef0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "ALTER TABLE call_center_scores\n",
    "SET TBLPROPERTIES (delta.enableChangeDataFeed = true)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6894855959288539,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "transcriptions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
